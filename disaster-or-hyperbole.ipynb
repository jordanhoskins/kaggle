{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2e3deb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:28.941579Z",
     "iopub.status.busy": "2023-02-03T17:52:28.941064Z",
     "iopub.status.idle": "2023-02-03T17:52:30.796200Z",
     "shell.execute_reply": "2023-02-03T17:52:30.794797Z"
    },
    "papermill": {
     "duration": 1.866844,
     "end_time": "2023-02-03T17:52:30.799588",
     "exception": false,
     "start_time": "2023-02-03T17:52:28.932744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "ids = test_df.id.values\n",
    "test_df.drop(\"id\", axis=1, inplace=True)\n",
    "train_df.drop(\"id\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "438aa721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:30.812618Z",
     "iopub.status.busy": "2023-02-03T17:52:30.811906Z",
     "iopub.status.idle": "2023-02-03T17:52:30.828023Z",
     "shell.execute_reply": "2023-02-03T17:52:30.826563Z"
    },
    "papermill": {
     "duration": 0.02563,
     "end_time": "2023-02-03T17:52:30.830758",
     "exception": false,
     "start_time": "2023-02-03T17:52:30.805128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatalities               45\n",
      "deluge                   42\n",
      "armageddon               42\n",
      "sinking                  41\n",
      "damage                   41\n",
      "                         ..\n",
      "forest%20fire            19\n",
      "epicentre                12\n",
      "threat                   11\n",
      "inundation               10\n",
      "radiation%20emergency     9\n",
      "Name: keyword, Length: 221, dtype: int64\n",
      "**************************************************\n",
      "USA                    104\n",
      "New York                71\n",
      "United States           50\n",
      "London                  45\n",
      "Canada                  29\n",
      "                      ... \n",
      "MontrÌ©al, QuÌ©bec       1\n",
      "Montreal                 1\n",
      "ÌÏT: 6.4682,3.18287      1\n",
      "Live4Heed??              1\n",
      "Lincoln                  1\n",
      "Name: location, Length: 3341, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the  keywords and locations\n",
    "\n",
    "kwds = train_df.keyword.value_counts()\n",
    "locs = train_df.location.value_counts()\n",
    "\n",
    "print(kwds)\n",
    "print(\"*\"*50)\n",
    "print(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd4091c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:30.843501Z",
     "iopub.status.busy": "2023-02-03T17:52:30.843022Z",
     "iopub.status.idle": "2023-02-03T17:52:31.921037Z",
     "shell.execute_reply": "2023-02-03T17:52:31.919513Z"
    },
    "papermill": {
     "duration": 1.087873,
     "end_time": "2023-02-03T17:52:31.924065",
     "exception": false,
     "start_time": "2023-02-03T17:52:30.836192",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              164\n",
       "usa                           108\n",
       "new york                       75\n",
       "london                         50\n",
       "united states                  50\n",
       "                             ... \n",
       "birmingham uk                   2\n",
       "lisbon portugal                 2\n",
       "washington dc charlotte nc      2\n",
       "alexandria va                   2\n",
       "kashmir                         2\n",
       "Name: location, Length: 519, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations are inconsistent in scope (countries, cities, GPS coords),\n",
    "# encoding issues with diacritics, and some plain weird values\n",
    "special_chars = re.compile(\"[!@#$%^&-\\*\\.\\-\\|_?/\\[\\]:\\-\\+]\")\n",
    "spaces = re.compile(\"\\s+\")\n",
    "lead_trail_space = re.compile(\"(^\\s|\\s$)\")\n",
    "nums = re.compile(\".*\\d\")\n",
    "non_letters = re.compile(\"\\W\")\n",
    "diacritics = re.compile(\"^(û|å).*\")\n",
    "links = re.compile(\"http\")\n",
    "\n",
    "\n",
    "def process_location(df):\n",
    "    df.location = df.location.str.replace(\"Ì©\", \"e\")\n",
    "    df.location = df.location.str.lower()\n",
    "    df.location = df.location.str.replace(nums, \"\")\n",
    "    df.location = df.location.str.replace(special_chars, \"\")\n",
    "    df.location = df.location.str.replace(spaces, \" \")\n",
    "    df.location = df.location.str.replace(\",\", \"\")\n",
    "    df.location = df.location.str.replace(diacritics, \"\")\n",
    "    df.location = df.location.str.replace(links, \"\")\n",
    "    \n",
    "    uncommon_locs = {k: None for k,v in Counter(df.location.values).items() if v == 1}\n",
    "    df.location.replace(uncommon_locs, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = process_location(train_df)\n",
    "test_df = process_location(test_df)\n",
    "train_df.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e84ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:31.937702Z",
     "iopub.status.busy": "2023-02-03T17:52:31.937232Z",
     "iopub.status.idle": "2023-02-03T17:52:32.035182Z",
     "shell.execute_reply": "2023-02-03T17:52:32.033952Z"
    },
    "papermill": {
     "duration": 0.108358,
     "end_time": "2023-02-03T17:52:32.038053",
     "exception": false,
     "start_time": "2023-02-03T17:52:31.929695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "storm         104\n",
       "disaster      103\n",
       "fire           90\n",
       "fires          88\n",
       "emergency      79\n",
       "             ... \n",
       "zone           24\n",
       "rescue         22\n",
       "epicentre      12\n",
       "threat         11\n",
       "inundation     10\n",
       "Name: keyword, Length: 205, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_keyword(df):\n",
    "    df.keyword = df.keyword.str.replace(\"Ì©\", \"e\")\n",
    "    df.keyword = df.keyword.str.lower()\n",
    "    df.keyword = df.keyword.str.replace(nums, \"\")\n",
    "    df.keyword = df.keyword.str.replace(special_chars, \"\")\n",
    "    df.keyword = df.keyword.str.replace(spaces, \" \")\n",
    "    df.keyword = df.keyword.str.replace(\",\", \"\")\n",
    "    df.keyword = df.keyword.str.replace(diacritics, \"\")\n",
    "    df.keyword = df.keyword.str.replace(links, \"\")\n",
    "    \n",
    "    uncommon = {k: None for k,v in Counter(df.keyword.values).items() if v == 1}\n",
    "    df.keyword.replace(uncommon, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = process_keyword(train_df)\n",
    "test_df = process_keyword(test_df)\n",
    "train_df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5e9000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:32.052781Z",
     "iopub.status.busy": "2023-02-03T17:52:32.052039Z",
     "iopub.status.idle": "2023-02-03T17:52:32.114812Z",
     "shell.execute_reply": "2023-02-03T17:52:32.113799Z"
    },
    "papermill": {
     "duration": 0.073017,
     "end_time": "2023-02-03T17:52:32.117511",
     "exception": false,
     "start_time": "2023-02-03T17:52:32.044494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    ariaahrary thetawniest the out of control wild...\n",
       "7610    m194 0104 utc5km s of volcano hawaii tcozdtoyd...\n",
       "7611    police investigating after an ebike collided w...\n",
       "7612    the latest more homes razed by northern califo...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Process the text in much the same way as the location. This time we'll keep the \n",
    "hashtags as they could prove valuable\"\"\"\n",
    "special_chars = re.compile(\"[!@$%^&-\\*\\.\\-\\|_?/\\[\\]:\\-\\+]\")\n",
    "\n",
    "def process_text(df):\n",
    "    df.text = df.text.str.replace(special_chars, \"\")\n",
    "    df.text = df.text.str.lower()\n",
    "    df.text = df.text.str.replace(links, \"\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "train_df = process_text(train_df)\n",
    "test_df = process_text(test_df)\n",
    "\n",
    "train_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a79843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:32.131778Z",
     "iopub.status.busy": "2023-02-03T17:52:32.130931Z",
     "iopub.status.idle": "2023-02-03T17:52:32.229389Z",
     "shell.execute_reply": "2023-02-03T17:52:32.227925Z"
    },
    "papermill": {
     "duration": 0.108764,
     "end_time": "2023-02-03T17:52:32.232486",
     "exception": false,
     "start_time": "2023-02-03T17:52:32.123722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-hot encode the categorical variables\n",
    "cat_vars = [\"location\", \"keyword\"]\n",
    "\n",
    "def one_hot_encode(colname, df):\n",
    "    one_hot = pd.get_dummies(df[f\"{colname}\"], prefix=f\"{colname}\")\n",
    "    df.drop(colname, axis=1, inplace=True)\n",
    "    df = df.join(one_hot)\n",
    "    return df\n",
    "\n",
    "for var in cat_vars:\n",
    "    train_df = one_hot_encode(var, train_df)\n",
    "    test_df = one_hot_encode(var, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2aed43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:32.247008Z",
     "iopub.status.busy": "2023-02-03T17:52:32.246577Z",
     "iopub.status.idle": "2023-02-03T17:52:33.228545Z",
     "shell.execute_reply": "2023-02-03T17:52:33.227352Z"
    },
    "papermill": {
     "duration": 0.993666,
     "end_time": "2023-02-03T17:52:33.232244",
     "exception": false,
     "start_time": "2023-02-03T17:52:32.238578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Let's implement term frequency inverse document frequency from scratch because why not\n",
    "Term frequency - whether the term appears in the current document\n",
    "Inverse doc frequency - positive docs containing this term / all positive docs\n",
    "\"\"\"\n",
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "all_docs = pd.Series(\n",
    "    set(doc.split()) - stopwords\n",
    "    for doc in train_df.text\n",
    ")\n",
    "pos_docs = pd.Series(\n",
    "    set(doc.split()) - stopwords\n",
    "    for doc in train_df[train_df.target == 1].text\n",
    ")\n",
    "neg_docs = pd.Series(\n",
    "    set(doc.split()) - stopwords\n",
    "    for doc in train_df[train_df.target == 0].text\n",
    ")\n",
    "\n",
    "pos_terms = [word for doc in pos_docs for word in doc]\n",
    "pos_terms = {k for k, v in Counter(pos_terms).items() if v >= 6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0310b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:33.246446Z",
     "iopub.status.busy": "2023-02-03T17:52:33.245953Z",
     "iopub.status.idle": "2023-02-03T17:52:33.252763Z",
     "shell.execute_reply": "2023-02-03T17:52:33.251626Z"
    },
    "papermill": {
     "duration": 0.017038,
     "end_time": "2023-02-03T17:52:33.255165",
     "exception": false,
     "start_time": "2023-02-03T17:52:33.238127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Term:\n",
    "    def __init__(self, term, docs):\n",
    "        self.term = term\n",
    "        total_docs = len(docs)\n",
    "        docs_with_term = len(pd.Series(i for i in docs if term in i))\n",
    "        self.doc_freq = docs_with_term / total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e0e2e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:33.269441Z",
     "iopub.status.busy": "2023-02-03T17:52:33.268781Z",
     "iopub.status.idle": "2023-02-03T17:52:34.054090Z",
     "shell.execute_reply": "2023-02-03T17:52:34.053149Z"
    },
    "papermill": {
     "duration": 0.795422,
     "end_time": "2023-02-03T17:52:34.056709",
     "exception": false,
     "start_time": "2023-02-03T17:52:33.261287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate document frequencies for all terms\n",
    "term_freqs = [\n",
    "    Term(term=t, docs=pos_docs) for t in pos_terms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f2f8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:34.071584Z",
     "iopub.status.busy": "2023-02-03T17:52:34.070936Z",
     "iopub.status.idle": "2023-02-03T17:52:44.758843Z",
     "shell.execute_reply": "2023-02-03T17:52:44.757324Z"
    },
    "papermill": {
     "duration": 10.698774,
     "end_time": "2023-02-03T17:52:44.761771",
     "exception": false,
     "start_time": "2023-02-03T17:52:34.062997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Add each term as a column in the dataframe\n",
    "\n",
    "for t in term_freqs:\n",
    "    train_df[f\"term_{t.term}\"] = train_df.text.str.contains(t.term).astype(int)\n",
    "    test_df[f\"term_{t.term}\"] = test_df.text.str.contains(t.term).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b98096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:44.775749Z",
     "iopub.status.busy": "2023-02-03T17:52:44.775322Z",
     "iopub.status.idle": "2023-02-03T17:52:44.957979Z",
     "shell.execute_reply": "2023-02-03T17:52:44.956583Z"
    },
    "papermill": {
     "duration": 0.193368,
     "end_time": "2023-02-03T17:52:44.961161",
     "exception": false,
     "start_time": "2023-02-03T17:52:44.767793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.drop(\"text\", axis=1, inplace=True)\n",
    "test_df.drop(\"text\", axis=1, inplace=True)\n",
    "train_df.fillna(0, axis=1, inplace=True)\n",
    "test_df.fillna(0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb0396",
   "metadata": {
    "papermill": {
     "duration": 0.005339,
     "end_time": "2023-02-03T17:52:44.972287",
     "exception": false,
     "start_time": "2023-02-03T17:52:44.966948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ed847bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:44.985480Z",
     "iopub.status.busy": "2023-02-03T17:52:44.985042Z",
     "iopub.status.idle": "2023-02-03T17:52:45.189222Z",
     "shell.execute_reply": "2023-02-03T17:52:45.188192Z"
    },
    "papermill": {
     "duration": 0.21386,
     "end_time": "2023-02-03T17:52:45.191833",
     "exception": false,
     "start_time": "2023-02-03T17:52:44.977973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"The one-hot encoding process has added some columns unique to each dataset. We'll substitute 0s\n",
    "for these unseen features in both train and test. This is necessary because without it, the predictor\n",
    "will encounter feature vectors that it hasn't been trained with. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_feats = set([i for i in train_df.columns if i != \"target\"])\n",
    "test_feats = set(test_df.columns)\n",
    "\n",
    "feats_not_in_train = list(test_feats-train_feats)\n",
    "dummy_df = pd.DataFrame.from_records([{col: 0} for col in feats_not_in_train])\n",
    "train_df = pd.concat([train_df, dummy_df], axis=1)\n",
    "train_df.fillna(0, inplace=True)\n",
    "\n",
    "feats_not_in_test = list(train_feats - test_feats)\n",
    "dummy_df = pd.DataFrame.from_records([{col: 0} for col in feats_not_in_test])\n",
    "test_df = pd.concat([test_df, dummy_df], axis=1)\n",
    "test_df.fillna(0, inplace=True)\n",
    "\n",
    "# Make sure the feature columns ordered the same in train/test\n",
    "# Otherwise, they lose their meaning\n",
    "train_df = train_df.reindex(sorted(train_df.columns), axis=1)\n",
    "test_df = test_df.reindex(sorted(test_df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3bd337b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:45.205281Z",
     "iopub.status.busy": "2023-02-03T17:52:45.204839Z",
     "iopub.status.idle": "2023-02-03T17:52:45.335062Z",
     "shell.execute_reply": "2023-02-03T17:52:45.333897Z"
    },
    "papermill": {
     "duration": 0.14025,
     "end_time": "2023-02-03T17:52:45.337959",
     "exception": false,
     "start_time": "2023-02-03T17:52:45.197709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create train/test split for eval\"\"\"\n",
    "X = train_df[[i for i in train_df.columns if i != \"target\"]]\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels  = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c010673",
   "metadata": {
    "papermill": {
     "duration": 0.005373,
     "end_time": "2023-02-03T17:52:45.349294",
     "exception": false,
     "start_time": "2023-02-03T17:52:45.343921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Norms supported by each logistic regression solver\n",
    "\n",
    "‘lbfgs’ - [‘l2’, None]\n",
    "\n",
    "‘liblinear’ - [‘l1’, ‘l2’]\n",
    "\n",
    "‘newton-cg’ - [‘l2’, None]\n",
    "\n",
    "‘newton-cholesky’ - [‘l2’, None]\n",
    "\n",
    "‘sag’ - [‘l2’, None]\n",
    "\n",
    "‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab17a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:45.363318Z",
     "iopub.status.busy": "2023-02-03T17:52:45.362877Z",
     "iopub.status.idle": "2023-02-03T17:52:46.071408Z",
     "shell.execute_reply": "2023-02-03T17:52:46.069699Z"
    },
    "papermill": {
     "duration": 0.720353,
     "end_time": "2023-02-03T17:52:46.076055",
     "exception": false,
     "start_time": "2023-02-03T17:52:45.355702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.7847769028871391, 'tp': 109, 'tn': 190, 'fp': 26, 'fn': 56}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate logistic regression model \n",
    "params = {\n",
    "    \"random_state\": 42,\n",
    "    \"penalty\": \"l1\",\n",
    "    \"solver\": \"liblinear\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"l1_ratio\": None\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(**params).fit(train_data, train_labels)\n",
    "preds = log_reg.predict(test_data)\n",
    "tn, fp, fn, tp = confusion_matrix(test_labels, preds).ravel()\n",
    "acc = accuracy_score(test_labels, preds)\n",
    "scores = {\n",
    "    \"acc\": acc,\n",
    "    \"tp\": tp,\n",
    "    \"tn\": tn,\n",
    "    \"fp\": fp,\n",
    "    \"fn\": fn\n",
    "}\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6961d347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T17:52:46.108546Z",
     "iopub.status.busy": "2023-02-03T17:52:46.107752Z",
     "iopub.status.idle": "2023-02-03T17:52:46.217061Z",
     "shell.execute_reply": "2023-02-03T17:52:46.215275Z"
    },
    "papermill": {
     "duration": 0.131395,
     "end_time": "2023-02-03T17:52:46.222061",
     "exception": false,
     "start_time": "2023-02-03T17:52:46.090666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "answer_preds = log_reg.predict(test_df)\n",
    "d = {\"id\": ids, \"target\": answer_preds}\n",
    "answer_df = pd.DataFrame(d)\n",
    "answer_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c2c6c",
   "metadata": {
    "papermill": {
     "duration": 0.014337,
     "end_time": "2023-02-03T17:52:46.251367",
     "exception": false,
     "start_time": "2023-02-03T17:52:46.237030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.54432,
   "end_time": "2023-02-03T17:52:47.197745",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-03T17:52:18.653425",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
